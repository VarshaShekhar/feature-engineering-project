{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "\n",
    "<h1 align=center><font size = 5>FEATURE ENGINEERING End-to End PROJECT (30M) </font></h1>\n",
    "<h2 align=center><font size = 5>AIML Certification Programme</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding (1M)\n",
    "\n",
    "Students are expected to identify a regression problem of your choice. You have to detail the Business Understanding part of your problem under this heading which basically addresses the following questions.\n",
    "\n",
    "   1. What is the business problem that you are trying to solve?\n",
    "   2. What data do you need to answer the above problem?What are the different sources of data?\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Business problem\n",
    "The goal of this project is to solve the following business problem:\n",
    "\n",
    "**\"How can we accurately predict the selling price of products listed on Mercari's marketplace using the product's attributes and listing details?\"**\n",
    "\n",
    "This problem is valuable to both:\n",
    "\n",
    "- **Sellers**: Accurate price predictions can help sellers list items competitively, leading to faster and more successful sales.\n",
    "- **Mercari**: Better pricing guidance enhances buyer trust, improves search relevance, and increases platform revenue by boosting completed transactions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2. What Data Is Needed?\n",
    "\n",
    "To solve this regression problem, we require a dataset containing:\n",
    "\n",
    "- **Target Variable**:\n",
    "  - `price` â€“ Final sale price of the product.\n",
    "\n",
    "- **Feature Variables**:\n",
    "  - `name` â€“ Product title.\n",
    "  - `item_description` â€“ Description of the product.\n",
    "  - `brand_name` â€“ Brand of the product.\n",
    "  - `category_name` â€“ Category path of the item (e.g., \"Electronics/Computers\").\n",
    "  - `item_condition_id` â€“ Condition rating provided by the seller.\n",
    "  - `shipping` â€“ Who pays for shipping (0 = buyer, 1 = seller).\n",
    "\n",
    "We may also engineer additional features like:\n",
    "- Text length or word count of `item_description`.\n",
    "- Whether the `brand_name` is missing.\n",
    "- First-level category extraction (e.g., \"Women\", \"Men\", \"Home\").\n",
    "\n",
    "---\n",
    "\n",
    "### Data Sources\n",
    "\n",
    "The dataset used for this project comes from:\n",
    "\n",
    "- **Mercari Price Suggestion Challenge** â€“ Kaggle  \n",
    "  Link: [https://www.kaggle.com/competitions/mercari-price-suggestion-challenge/data](https://www.kaggle.com/competitions/mercari-price-suggestion-challenge/data)\n",
    "\n",
    "If expanded further, external data could be considered, such as:\n",
    "- Market pricing data from similar marketplaces.\n",
    "- Brand/product reputation information from external APIs.\n",
    "- User behavior analytics (views, clicks, favorites).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Data Requirements and Data Collection (3+1M)<a id=\"0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DS0103EN/labs/images/lab2_fig1_flowchart_data_requirements.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In the initial data collection stage, data scientists identify and gather the available data resources. These can be in the form of structured, unstructured, and even semi-structured data relevant to the problem domain.\n",
    "\n",
    "Identify the required data that fulfills the data requirements stage of the data science methodology <br>\n",
    "<b> Mention the source of the data.(Give the link if you have sourced it from any public data set)\n",
    "Briefly explain the data set identified .</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identified Data\n",
    "\n",
    "To solve the regression problem of predicting the price of items on an e-commerce platform like Mercari, the required data must include:\n",
    "\n",
    "- A continuous target variable: `price`\n",
    "- A mix of **categorical** and **textual** features (e.g., brand, category, item condition)\n",
    "- Potentially useful engineered features from text and categories\n",
    "\n",
    "The dataset must allow for:\n",
    "- Data cleaning (e.g., missing values, inconsistencies)\n",
    "- Feature engineering (e.g., binning, encoding)\n",
    "- Regression modeling\n",
    "- Feature selection techniques like Chi-Squared and Mutual Information\n",
    "\n",
    "---\n",
    "\n",
    "### Data Source\n",
    "\n",
    "The data used in this project is sourced from a public competition:\n",
    "\n",
    "- **Dataset**: Mercari Price Suggestion Challenge  \n",
    "- **Platform**: [Kaggle](https://www.kaggle.com/competitions/mercari-price-suggestion-challenge/data)\n",
    "\n",
    "ðŸ”— [Click here to access the dataset](https://www.kaggle.com/competitions/mercari-price-suggestion-challenge/data)\n",
    "\n",
    "---\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "The Mercari dataset contains approximately **1.5 million** records of item listings, with the goal of predicting the **final sale price** of an item based on the product's listing details. Each record includes:\n",
    "\n",
    "- `train_id` â€“ Unique identifier\n",
    "- `name` â€“ Product title\n",
    "- `item_condition_id` â€“ Item's condition rating (1 to 5)\n",
    "- `category_name` â€“ Full product category (hierarchical string)\n",
    "- `brand_name` â€“ Product brand (may be missing)\n",
    "- `price` â€“ Target variable (continuous, USD)\n",
    "- `shipping` â€“ Who pays shipping (0 = buyer, 1 = seller)\n",
    "- `item_description` â€“ Free-text description of the item\n",
    "\n",
    "This dataset is ideal for demonstrating:\n",
    "- Data preprocessing and cleaning\n",
    "- Text feature engineering\n",
    "- Binning and discretization\n",
    "- Feature selection (Chi-Squared & Mutual Information)\n",
    "- Regression modeling using linear regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import chi2, mutual_info_regression, SelectKBest\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Import the above data and read it into a data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (Mercari data is tab-separated, not comma-separated!)\n",
    "df = pd.read_csv(\"train.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Sampling the data\n",
    "df = df.sample(n=10000, random_state=42).copy()\n",
    "# Compute description length\n",
    "df['desc_len'] = df['item_description'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Define y and X\n",
    "y = df['price']\n",
    "X = df.drop(columns=['price', 'name', 'item_description'], errors='ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Sampling and Feature Creation\n",
    "To ensure efficient processing and avoid memory-related issues due to the large original dataset (~1.4 million rows), we sampled 10,000 rows using a fixed random_state for reproducibility. This subset was used throughout the entire workflowâ€”from preprocessing to modeling.\n",
    "\n",
    "Additionally, we introduced a new feature called desc_len, which captures the number of words in the product description. This acts as a proxy for text content richness, potentially influencing price. The desc_len feature is computed during this initial stage and added directly to the dataset for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Confirm the data has been correctly by displaying the first 5 and last 5 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows\n",
    "print(\"First 5 records:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display the last 5 rows\n",
    "print(\"\\nLast 5 records:\")\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Get the dimensions of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dimensions of the DataFrame\n",
    "print(\"DataFrame dimensions (rows, columns):\")\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Display the description and statistical summary of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of data\n",
    "df.info()\n",
    "\n",
    "# Statistical summary of numerical columns\n",
    "print(\"Statistical Summary (Numerical Columns):\")\n",
    "print(df.describe())\n",
    "\n",
    "# Including summary of object (categorical/text) columns:\n",
    "print(\"\\nStatistical Summary (All Columns Including Object Types):\")\n",
    "print(df.describe(include='all'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Display the columns and their respective data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column names and their data types\n",
    "print(\"Columns and Data Types (as DataFrame):\")\n",
    "print(pd.DataFrame({'Column': df.columns, 'Data Type': df.dtypes.values}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the columns to appropriate data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data types appropriately\n",
    "df['name'] = df['name'].astype(str)\n",
    "df['item_description'] = df['item_description'].astype(str)\n",
    "df['category_name'] = df['category_name'].astype('category')\n",
    "df['brand_name'] = df['brand_name'].astype('category')\n",
    "df['shipping'] = df['shipping'].astype('int8')\n",
    "df['item_condition_id'] = df['item_condition_id'].astype('int8')\n",
    "df['desc_len'] = df['desc_len'].astype('float32')\n",
    "df['price'] = df['price'].astype('float32') \n",
    "\n",
    "# Optional ID field\n",
    "if 'train_id' in df.columns:\n",
    "    df['train_id'] = df['train_id'].astype('int32')\n",
    "elif 'test_id' in df.columns:\n",
    "    df['test_id'] = df['test_id'].astype('int32')\n",
    "\n",
    "print(\"Data types converted successfully.\")\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your observations from the above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Observations**\n",
    "\n",
    "1. **Categorical Data Identified**:\n",
    "\n",
    "   * Columns like `category_name` and `brand_name` were converted to `category` type.\n",
    "   * This reduces memory usage and prepares them for encoding (e.g., one-hot or label encoding).\n",
    "\n",
    "2. **Textual Data Recognized**:\n",
    "\n",
    "   * `name` and `item_description` are stored as strings.\n",
    "   * These will likely require text preprocessing (e.g., tokenization, TF-IDF) before use in modeling.\n",
    "\n",
    "3. **Numerical Columns Optimized**:\n",
    "\n",
    "   * `shipping` and `item_condition_id` were converted to smaller integer types (`int8`) to optimize memory.\n",
    "   * Convert `desc_len` to `float32` so it works correctly with preprocessing and ML models, which expect numeric data with continuous values.\n",
    "   * `price` was set to `float32`, sufficient for precision while reducing memory compared to `float64`.\n",
    "\n",
    "4. **ID Columns Cast Properly**:\n",
    "\n",
    "   * ID column like `train_id` was cast to `int32`, appropriate for identifiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Data Quality Issues (1.5M)\n",
    "\n",
    "* duplicate data\n",
    "* missing data\n",
    "* data inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_rows = df[df.duplicated()]\n",
    "print(f\"Number of duplicate rows: {duplicate_rows.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Percentage of missing data\n",
    "missing_percent = (df.isnull().sum() / len(df)) * 100\n",
    "print(\"\\nPercentage of missing values per column:\")\n",
    "print(missing_percent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for blank strings in 'name' and 'item_description'\n",
    "blank_names = df['name'].str.strip() == ''\n",
    "blank_descriptions = df['item_description'].str.strip() == ''\n",
    "\n",
    "print(f\"Blank 'name' entries: {blank_names.sum()}\")\n",
    "print(f\"Blank 'item_description' entries: {blank_descriptions.sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling the data quality issues(1.5M)\n",
    "Apply techniques\n",
    "* to remove duplicate data\n",
    "* to impute or remove missing data\n",
    "* to remove data inconsistencies <br>\n",
    "Give detailed explanation for each column how you handle the data quality issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Remove duplicate data\n",
    "# Before removing duplicates\n",
    "print(\"Before duplicate removal:\", df.shape)\n",
    "\n",
    "# Remove duplicates based on unique ID \n",
    "id_col = 'train_id' if 'train_id' in df.columns else 'test_id'\n",
    "df.drop_duplicates(subset=id_col, inplace=True)\n",
    "\n",
    "# After removing duplicates\n",
    "print(\"After duplicate removal:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Impute or remove missing data\n",
    "# View missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Example strategy: fill numeric columns with median\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "\n",
    "# Ensure brand_name is categorical\n",
    "df['brand_name'] = df['brand_name'].astype('category')\n",
    "\n",
    "# Add 'Unknown' category if it does not exist\n",
    "if 'Unknown' not in df['brand_name'].cat.categories:\n",
    "    df['brand_name'] = df['brand_name'].cat.add_categories(['Unknown'])\n",
    "\n",
    "# Now fill missing values\n",
    "df['brand_name'] = df['brand_name'].fillna('Unknown')\n",
    "\n",
    "# # Ensure category_name is categorical\n",
    "# df['category_name'] = df['category_name'].astype('category')\n",
    "\n",
    "# # Add 'unknown/unknown/unknown' only if it's not already in the categories\n",
    "# if 'unknown/unknown/unknown' not in df['category_name'].cat.categories:\n",
    "#     df['category_name'] = df['category_name'].cat.add_categories(['unknown/unknown/unknown'])\n",
    "\n",
    "# # Now fill missing values\n",
    "# df['category_name'] = df['category_name'].fillna('unknown/unknown/unknown')\n",
    "# # Split into 3 subcategories\n",
    "categories = df['category_name'].str.split('/', expand=True)\n",
    "df['cat_1'] = categories[0].astype('category')\n",
    "df['cat_2'] = categories[1].astype('category')\n",
    "df['cat_3'] = categories[2].astype('category')\n",
    "\n",
    "# # Categorical columns â€“ Fill remaining with \"Unknown\"\n",
    "# cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "# df[cat_cols] = df[cat_cols].fillna(\"Unknown\")\n",
    "cat_cols = df.select_dtypes(include=['category']).columns\n",
    "obj_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# For categorical columns: add 'Unknown' category first, then fillna\n",
    "for col in cat_cols:\n",
    "    if 'Unknown' not in df[col].cat.categories:\n",
    "        df[col] = df[col].cat.add_categories(['Unknown'])\n",
    "    df[col] = df[col].fillna('Unknown')\n",
    "\n",
    "# For object columns, fillna directly\n",
    "df[obj_cols] = df[obj_cols].fillna('Unknown')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) Remove data inconsistencies (column-wise explanation)\n",
    "# Strip whitespace and convert to lowercase in all object columns\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = df[col].str.strip().str.lower()\n",
    "\n",
    "# Fix 'item_description': replace known placeholder and fill NaN\n",
    "df['item_description'] = df['item_description'].replace('No description yet', np.nan)\n",
    "df['item_description'] = df['item_description'].fillna('no description')\n",
    "\n",
    "# Clean text fields\n",
    "df['name'] = df['name'].str.lower().str.strip()\n",
    "df['item_description'] = df['item_description'].str.lower().str.strip()\n",
    "df['item_description'] = df['item_description'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "# Sanitize item_condition_id (should be 1-5)\n",
    "df['item_condition_id'] = df['item_condition_id'].apply(lambda x: x if x in [1, 2, 3, 4, 5] else 3).astype('int8')\n",
    "\n",
    "# Validate shipping\n",
    "df['shipping'] = df['shipping'].apply(lambda x: 1 if x == 1 else 0).astype('int8')\n",
    "\n",
    "# Remove zero or negative prices if present\n",
    "if 'price' in df.columns:\n",
    "    df = df[df['price'] > 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "To prepare the Mercari dataset for modeling, we implemented the following systematic cleaning procedures:\n",
    "\n",
    "* **No Duplicate Data:**\n",
    "  Verified that there were no duplicate entries based on the unique identifier (`train_id` or `test_id`), ensuring each record is unique and preventing data leakage.\n",
    "\n",
    "* **Missing Value Imputation:**\n",
    "  Missing values in numeric columns were filled with the median values to maintain distribution integrity. For categorical columns such as `brand_name` and `category_name`, missing entries were filled with `\"Unknown\"` after adding it as a valid category to avoid losing records.\n",
    "\n",
    "* **Category Splitting:**\n",
    "  The hierarchical `category_name` string was split into three separate categorical columns (`cat_1`, `cat_2`, and `cat_3`) to better capture granular category information.\n",
    "\n",
    "* **Text Cleaning:**\n",
    "  All text columns (`name`, `item_description`) were standardized by converting to lowercase, stripping whitespace, and removing special characters to reduce noise and inconsistencies.\n",
    "\n",
    "* **Handling Placeholders:**\n",
    "  Common placeholders such as `\"No description yet\"` in `item_description` were replaced with a standard `\"no description\"` tag to signify missing descriptive content.\n",
    "\n",
    "* **Validation and Correction:**\n",
    "  Values in `item_condition_id` were constrained to valid integers (1 through 5), and `shipping` was converted to a strict binary indicator (0 or 1).\n",
    "\n",
    "* **Filtering Invalid Prices:**\n",
    "  Entries with zero or negative prices were excluded, ensuring the model trains on meaningful price values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardise the data (1M)\n",
    "Standardization is the process of transforming data into a common format which you to make the meaningful comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say you want to standardize numerical features\n",
    "features_to_standardize = ['price', 'desc_len']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_std = df.copy()\n",
    "df_std[features_to_standardize] = scaler.fit_transform(df_std[features_to_standardize])\n",
    "\n",
    "print(\"Standardized data:\")\n",
    "print(df_std[features_to_standardize].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on Standardization:\n",
    "Standardization transforms features to have a mean of zero and a standard deviation of one. \n",
    "This is essential for algorithms like Linear Regression to perform optimally, as it ensures all features are on the same scale, preventing dominance by features with larger magnitude. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise the data wherever necessary(1M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a copy of your original DataFrame to preserve raw data\n",
    "df_norm = df.copy()\n",
    "\n",
    "# Step 2: Log-transform skewed data (e.g., 'price')\n",
    "df_norm['log_price'] = np.log1p(df_norm['price'])  # Safe even for zero prices\n",
    "\n",
    "# Step 4: Define features to normalize (log-transformed and others)\n",
    "features_to_normalize = ['log_price', 'desc_len']\n",
    "\n",
    "# Step 5: Apply Min-Max normalization\n",
    "scaler = MinMaxScaler()\n",
    "# df_norm[[f + '_norm' for f in features_to_normalize]] = scaler.fit_transform(df_norm[features_to_normalize])\n",
    "df_norm['log_price_norm'] = scaler.fit_transform(df_norm[['log_price']])\n",
    "df_norm['desc_len_norm'] = scaler.fit_transform(df_norm[['desc_len']])\n",
    "\n",
    "# Step 6: View normalized columns\n",
    "print(\"Normalized Data Preview:\")\n",
    "print(df_norm['desc_len_norm'].describe())\n",
    "print(df_norm['log_price_norm'].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization Note:\n",
    "To ensure features are on a comparable scale, especially for algorithms sensitive to feature magnitude like regression , we applied a log transformation to the price feature to reduce right-skewness. This was followed by min-max normalization to scale it between 0 and 1. Additionally, we normalized the desc_len feature (description length) using the same method. Categorical and binary fields were excluded from normalization as they are better handled by encoding.This improves model stability and convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Binning (1M)\n",
    "Binning is a process of transforming continuous numerical variables into discrete categorical 'bins', for grouped analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equal-frequency Binning on price\n",
    "# Create price bins (quantile-based) - 4 bins: Q1, Q2, Q3, Q4\n",
    "\n",
    "# Number of bins\n",
    "num_bins = 4\n",
    "# df['price_bin'] = pd.qcut(df['price'], q=4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "df_norm['price_bin'] = pd.cut(df_norm['log_price_norm'], bins=num_bins, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "print(df_norm['price_bin'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning desc_len\n",
    "# Description length binning into 3 categories\n",
    "features_to_bin = ['desc_len_norm']\n",
    "bins=3\n",
    "for feat in features_to_bin:\n",
    "    df_norm[f'{feat}_bin'] = pd.cut(df_norm[feat], bins=bins, labels=['Short', 'Medium', 'Long'])\n",
    "\n",
    "print(df_norm[[f'{feat}_bin']].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on Binning\n",
    "To transform continuous numeric variables into categorical ones, we applied binning techniques that simplify the data distribution and help capture nonlinear relationships:\n",
    "\n",
    "#### Equal-frequency (quantile) binning for Price:\n",
    "\n",
    "The log_price_norm variable was divided into 4 bins labeled 'Low', 'Medium', 'High', and 'Very High'.\n",
    "\n",
    "This splits the data into roughly equal-sized groups based on price quantiles, allowing the model to learn from discrete price categories rather than raw continuous values.\n",
    "\n",
    "#### Binning Description Length (desc_len_norm):\n",
    "\n",
    "The normalized description length was segmented into 3 categories: 'Short', 'Medium', and 'Long'.\n",
    "\n",
    "This categorization helps capture the impact of product description verbosity on pricing in a simplified, interpretable form.\n",
    "\n",
    "#### Binning is especially useful when:\n",
    "\n",
    "* Handling skewed distributions,\n",
    "\n",
    "* Enabling models that benefit from categorical inputs,\n",
    "\n",
    "Improving interpretability by grouping continuous features into meaningful buckets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform encoding (1M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to one-hot encode (categorical bins, small categories)\n",
    "one_hot_cols = ['cat_1', 'cat_2', 'cat_3', 'price_bin', 'price_bin_custom', 'desc_bin']\n",
    "\n",
    "# Make sure these columns exist in df_norm\n",
    "one_hot_cols = [col for col in one_hot_cols if col in df_norm.columns]\n",
    "\n",
    "# One-hot encode the specified columns in normalized dataframe\n",
    "df_norm_encoded = pd.get_dummies(df_norm, columns=one_hot_cols, dummy_na=True)\n",
    "\n",
    "# Label encode brand_name in df_norm_encoded (brand can be many categories)\n",
    "le_brand = LabelEncoder()\n",
    "df_norm_encoded['brand_name_le'] = le_brand.fit_transform(df_norm_encoded['brand_name'].astype(str))\n",
    "\n",
    "# Optionally drop original brand_name to avoid duplication\n",
    "df_norm_encoded.drop(columns=['brand_name'], inplace=True)\n",
    "\n",
    "print(\"After encoding, df_norm_encoded shape:\", df_norm_encoded.shape)\n",
    "print(df_norm_encoded.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on Encoding\n",
    "#### One-Hot Encoding:\n",
    "Selected categorical columns with relatively small numbers of unique categories (including binned features like price and description length) were converted into one-hot encoded vectors.\n",
    "This process creates binary indicator columns for each category level, allowing the model to interpret categorical data numerically without implying any ordinal relationship.\n",
    "\n",
    "#### Label Encoding for brand_name:\n",
    "Since brand_name contains many unique values (high cardinality), one-hot encoding would create a very large sparse matrix. Instead, it was label encoded into integer codes to reduce dimensionality while still distinguishing different brands.\n",
    "\n",
    "#### Handling Missing Categories:\n",
    "The dummy_na=True parameter in get_dummies adds an extra indicator column for missing values, ensuring that missing data in categorical columns is explicitly represented.\n",
    "\n",
    "#### Dropping Original Columns:\n",
    "After encoding, original categorical columns are dropped to avoid duplication and maintain a consistent numeric feature set for modeling.\n",
    "\n",
    "This combination of encoding techniques balances memory efficiency and model interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Data Discretization(2M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equal-frequency Discretization on price (quantile-based)\n",
    "num_bins = 4\n",
    "df_norm['price_disc'] = pd.qcut(df_norm['log_price_norm'], q=num_bins, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "# Equal-frequency Discretization on description length\n",
    "bins = 3\n",
    "df_norm['desc_len_disc'] = pd.qcut(df_norm['desc_len'], q=bins, labels=['Short', 'Medium', 'Long'])\n",
    "\n",
    "# Check distribution\n",
    "print(df_norm['price_disc'].value_counts().sort_index())\n",
    "print(df_norm['desc_len_disc'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on Data Discretization\n",
    "\n",
    "To convert continuous numerical variables into categorical bins for easier interpretation and modeling, we applied **equal-frequency discretization (quantile-based binning)**:\n",
    "\n",
    "* **Price Binning (`price_disc`)**:\n",
    "  The normalized log-transformed price (`log_price_norm`) was divided into **4 bins** representing roughly equal-sized groups labeled **Low**, **Medium**, **High**, and **Very High**. This approach helps capture price tiers and can improve model interpretability when used as categorical features.\n",
    "\n",
    "* **Description Length Binning (`desc_len_disc`)**:\n",
    "  The raw description length (`desc_len`) was split into **3 bins** labeled **Short**, **Medium**, and **Long**, representing the relative verbosity of product descriptions. This categorical transformation enables the model to leverage description size as a meaningful discrete feature.\n",
    "\n",
    "This ensures that each category contains approximately the same number of samples, which balances the data distribution across bins and reduces bias toward any particular range.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA using Visuals(3M)\n",
    "Use any 3 or more visualisation methods (Boxplot,Scatterplot,histogram,....etc) to perform Exploratory data analysis and briefly give interpretations from each visual.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df['price'], bins=50, kde=False)\n",
    "plt.title('Histogram of Price')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Count')\n",
    "plt.xlim(0, 500)  # Limit x-axis to focus on most data\n",
    "plt.show()\n",
    "\n",
    "# Log-scale histogram (to visualize skew better)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(np.log1p(df['price']), bins=50, kde=True)\n",
    "plt.title('Histogram of Log(Price + 1)')\n",
    "plt.xlabel('Log(Price + 1)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Histogram Analysis of Product Prices**\n",
    "\n",
    "To understand the distribution of product prices in the dataset, two histograms were created â€” one using the raw price values and another using the log-transformed price (`log(price + 1)`).\n",
    "\n",
    "#### 1. Histogram of Raw Prices\n",
    "\n",
    "* The distribution of raw prices is **highly right-skewed**.\n",
    "* **Most products are priced below â‚¹100**, with a sharp drop in frequency as prices increase.\n",
    "* There are a **small number of high-priced outliers**, which makes it harder for some models to learn effectively from the data.\n",
    "\n",
    "#### 2. Histogram of Log(Price + 1)\n",
    "\n",
    "* After applying **log transformation**, the distribution becomes **much more symmetrical and nearly normal**.\n",
    "* The transformation reduces the effect of outliers and **compresses the range**, making the data more suitable for linear regression and other statistical models.\n",
    "* This transformation improves **model stability and performance** when price is used as a target variable.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion:**\n",
    "\n",
    "Log-transforming the price helps in **handling skewness**, **reducing the influence of outliers**, and is an **essential preprocessing step for regression-based modeling**. This ensures that assumptions like linearity and normality of residuals are better met.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='cat_1', y='price', data=df_norm)\n",
    "plt.title('Boxplot of Price by Top-Level Category (cat_1)')\n",
    "plt.xlabel('Top Category (cat_1)')\n",
    "plt.ylabel('Price')\n",
    "plt.ylim(0, df_norm['price'].quantile(0.95))  # Limit y-axis to 95th percentile price\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot of Price by Category:\n",
    "\n",
    "The boxplot visualizes the distribution of product prices across different top-level categories (cat_1). Each box represents the interquartile range (IQR) of prices within a category, with the median marked inside the box. Whiskers show the range excluding outliers, and points outside are considered price outliers.\n",
    "\n",
    "#### Insights from the plot:\n",
    "\n",
    "* It highlights how price varies significantly between categories.\n",
    "\n",
    "* Some categories show higher median prices and wider spread, indicating more variability.\n",
    "\n",
    "* Outliers reveal extreme price listings, useful for identifying unusual products or pricing errors.\n",
    "\n",
    "* This visualization aids in understanding category-based price patterns and helps in feature selection or further analysis.\n",
    "\n",
    "#### Note\n",
    "To improve readability and avoid extreme price outliers squishing the plot, the y-axis is limited to the 95th percentile of prices. This focuses the view on the majority of the data, making category-wise price differences clearer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Scatterplot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df_norm, x='desc_len', y='price', alpha=0.4)\n",
    "plt.title('Scatter Plot: Description Length vs. Price')\n",
    "plt.xlabel('Description Length (word count)')\n",
    "plt.ylabel('Price')\n",
    "plt.xlim(0, 300)  # optional: limit for better focus\n",
    "plt.ylim(0, 500)  # optional: filter high price outliers\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plot: Description Length vs. Price\n",
    "This scatter plot shows the relationship between the length of the item description (desc_len, measured in word count) and the product price. To focus on the bulk of the data and reduce the influence of extreme outliers:\n",
    "\n",
    "* The x-axis is limited to descriptions up to 300 words.\n",
    "\n",
    "* The y-axis is limited to prices up to $500.\n",
    "\n",
    "These limits help visualize the main data trend without being skewed by very long descriptions or very expensive items.\n",
    "\n",
    "#### Observations from the plot may include:\n",
    "\n",
    "* Items with very short descriptions tend to have lower prices.\n",
    "\n",
    "* There is a slight positive correlation where longer descriptions generally associate with higher prices.\n",
    "\n",
    "* Price variability increases with longer descriptions, indicating diverse product types or quality.\n",
    "\n",
    "* This insight justifies including desc_len as a predictive feature in the pricing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns for correlation\n",
    "numeric_cols = ['price', 'desc_len', 'log_price', 'log_price_norm', 'desc_len_norm', 'shipping', 'item_condition_id']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "corr_matrix = df_norm[numeric_cols].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Heatmap of Key Numeric Features\n",
    "This heatmap visualizes the Pearson correlation coefficients between selected numeric features in the dataset, including:\n",
    "\n",
    "- price and its log-transformed variants (log_price, log_price_norm)\n",
    "\n",
    "- Description length (desc_len and normalized desc_len_norm)\n",
    "\n",
    "- Shipping status (shipping)\n",
    "\n",
    "- Item condition (item_condition_id)\n",
    "\n",
    "#### Key takeaways:\n",
    "\n",
    "- Strong positive correlations exist between price and its log-transformed versions, confirming the effectiveness of log transformation in stabilizing variance.\n",
    "\n",
    "- Description length shows a modest positive correlation with price, supporting its predictive value.\n",
    "\n",
    "- Shipping and item condition have relatively weaker correlations with price, but still provide useful information.\n",
    "\n",
    "- Normalized features maintain similar relationships, validating the preprocessing steps.\n",
    "\n",
    "This heatmap aids in understanding feature relationships and guiding feature selection for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Visualizations\n",
    "\n",
    "* **Boxplot**: Highlights distribution differences and potential outliers across categorical groups, helping to compare price variations by category.\n",
    "\n",
    "* **Histogram**: Reveals the overall data distribution and skewness, providing insight into how features like price or description length are spread.\n",
    "\n",
    "* **Scatterplot**: Shows relationships and potential trends between two continuous variables, such as description length and price.\n",
    "\n",
    "* **Correlation Heatmap**: Quantifies the strength and direction of linear relationships between numeric features, guiding feature selection and understanding dependencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection(2M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Univariate filters identify top 5 significant features by evaluating each feature independently with respect to the target variable by exploring\n",
    "1. Mutual Information (Information Gain)\n",
    "2. Gini index\n",
    "3. Gain Ratio\n",
    "4. Chi-Squared test\n",
    "5. Fisher Score\n",
    "<br>(From the above 5 you are required to use any <b>two</b>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Preparation and Encoding for Modeling\n",
    "\n",
    "* **Feature Selection:** Removed text-heavy columns like `item_description` and `name` that are not directly usable in numeric models.\n",
    "\n",
    "* **Categorical Encoding:** Converted all categorical variables into numeric format using **one-hot encoding** with `drop_first=True` to avoid multicollinearity.\n",
    "\n",
    "* **Target Variable:** Defined the target variable `y_binned` as the binned (categorical) version of price for classification or analysis purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features (drop text columns like 'item_description', 'name')\n",
    "features = df_norm.drop(columns=['price', 'item_description', 'name'])\n",
    "\n",
    "# One-hot encode all categorical columns\n",
    "features_encoded = pd.get_dummies(features, drop_first=True)\n",
    "\n",
    "cat_cols = features.select_dtypes(include=['object', 'category']).columns\n",
    "features[cat_cols] = features[cat_cols].astype(str)  # ensure all are strings\n",
    "features_encoded = pd.get_dummies(features, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# Assuming 'price_bin' is your binned target column (categorical)\n",
    "y_binned = df_norm['price_bin']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Square Feature Selection\n",
    "\n",
    "* **Purpose:** Chi-Square (Ï‡Â²) test is used here to identify the most relevant categorical features that have the strongest association with the binned price target.\n",
    "\n",
    "* **Process:**\n",
    "\n",
    "  * Applied `SelectKBest` with `chi2` as the scoring function to select the top 10 features.\n",
    "  * The test evaluates the dependency between each feature and the target, highlighting features that are statistically significant predictors.\n",
    "\n",
    "* **Outcome:**\n",
    "\n",
    "  * Features with the highest Chi-Square scores are selected as the most informative for the price prediction model.\n",
    "  * This reduces dimensionality, improves model interpretability, and can enhance predictive performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Chi2 feature selection (k = number of features to select, e.g., 10)\n",
    "selector = SelectKBest(score_func=chi2, k=10)\n",
    "\n",
    "# Fit selector on encoded features and binned target\n",
    "selector.fit(features_encoded, y_binned)\n",
    "\n",
    "# Get scores and selected feature names\n",
    "chi2_scores = selector.scores_\n",
    "selected_features = features_encoded.columns[selector.get_support()]\n",
    "\n",
    "# Show results\n",
    "chi2_results = pd.DataFrame({'Feature': features_encoded.columns, 'Chi2_Score': chi2_scores})\n",
    "chi2_results = chi2_results.sort_values(by='Chi2_Score', ascending=False)\n",
    "\n",
    "print(chi2_results.head(10))\n",
    "print(\"\\nSelected Features:\\n\", selected_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information Feature Selection\n",
    "\n",
    "* **Purpose:** Mutual Information (MI) measures the amount of shared information between each feature and the continuous target (`log_price`), capturing both linear and non-linear dependencies.\n",
    "\n",
    "* **Process:**\n",
    "\n",
    "  * Computed MI scores using `mutual_info_regression` on one-hot encoded features against the log-transformed price.\n",
    "  * This method identifies features that provide the most predictive information about price variability.\n",
    "\n",
    "* **Outcome:**\n",
    "\n",
    "  * Features with the highest MI scores are considered most informative for predicting product prices.\n",
    "  * MI helps uncover complex relationships that may not be detected by linear methods like Chi-Square, supporting better feature selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your continuous target variable (use 'price' or 'log_price' â€” e.g., log_price for better distribution)\n",
    "y = df_norm['log_price']\n",
    "\n",
    "# Make sure features_encoded is your one-hot encoded feature dataframe\n",
    "# It should be all numeric and aligned with y\n",
    "X = features_encoded\n",
    "\n",
    "# Compute mutual information scores\n",
    "mi_scores = mutual_info_regression(X, y, discrete_features='auto', random_state=42)\n",
    "\n",
    "# Create a DataFrame to view results clearly\n",
    "mi_results = pd.DataFrame({'Feature': X.columns, 'MI_Score': mi_scores})\n",
    "mi_results = mi_results.sort_values(by='MI_Score', ascending=False)\n",
    "\n",
    "# Display top features\n",
    "print(mi_results.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report observations (2M)\n",
    "\n",
    "Write your observations from the results of each of the above method(1M). Clearly justify your choice of the method.(1M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Chi-Square (Chi2) Results:**\n",
    "\n",
    "* Chi2 identified several categorical features with significant associations to the target by measuring dependence between feature categories and binned target values.\n",
    "* However, since Chi2 requires discrete data and a categorical target, the continuous target variable was discretized (binned) to apply this method.\n",
    "* This discretization may lead to some loss of information and Chi2 cannot capture relationships with continuous or numerical features well.\n",
    "* Some important continuous features were not effectively ranked by Chi2 due to this limitation.\n",
    "\n",
    "#### **Mutual Information Regression (MI) Results:**\n",
    "\n",
    "* MI was able to measure the dependency between all types of features (both continuous and categorical, after encoding) and the continuous target directly.\n",
    "* It captured both linear and nonlinear relationships without the need to bin the target.\n",
    "* MI highlighted a broader set of relevant features, including numerical features that Chi2 missed.\n",
    "* The results provided a more nuanced understanding of which features contribute most to predicting the target variable.\n",
    "\n",
    "### Justification of Method Choice\n",
    "\n",
    "While Chi-Square is useful for understanding relationships in categorical data, Mutual Information Regression is more suitable for this regression problem because it directly handles continuous targets and captures complex dependencies. Therefore, **MI was chosen as the primary feature selection method** to retain features that have the strongest information content for predicting the continuous target. Chi2 was used additionally to compare and validate the importance of categorical features after discretization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis (3 M)\n",
    "Perform correlation analysis(1M) and plot the visuals(1M).Briefly explain each process,why is it used and interpret the result(1M)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Calculation:\n",
    "We calculate Pearson correlation coefficients between numeric features. This measures the strength and direction of a linear relationship between pairs of variables, with values ranging from -1 (perfect negative correlation) to +1 (perfect positive correlation), and 0 indicating no linear correlation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select numeric columns for correlation analysis\n",
    "numeric_cols = df_norm.select_dtypes(include=['float64', 'int64', 'int8', 'float32']).columns\n",
    "\n",
    "# For example, consider relevant numeric features only\n",
    "corr_features = ['price', 'log_price_norm', 'desc_len_norm', 'item_condition_id', 'shipping']\n",
    "\n",
    "# 2. Calculate correlation matrix\n",
    "corr_matrix = df_norm[corr_features].corr()\n",
    "\n",
    "print(\"Correlation matrix:\\n\", corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap Visualization:\n",
    "A heatmap visually represents the correlation matrix using colors. Positive correlations are shown in warm colors (e.g., red), negative correlations in cool colors (e.g., blue). Annotations show exact correlation values, making it easier to interpret relationships at a glance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap of Selected Numeric Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Correlation Analysis?\n",
    "\n",
    "- Helps identify redundant features (highly correlated variables) which might be dropped during feature selection.\n",
    "\n",
    "- Detects multicollinearity issues that could affect regression models.\n",
    "\n",
    "- Reveals potential linear associations useful for predictive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of Correlation Matrix:\n",
    "\n",
    "* **Price & Log Price Norm (0.737):**\n",
    "  There is a strong positive correlation between `price` and its normalized log-transformed version (`log_price_norm`). This is expected since `log_price_norm` is a transformed representation of `price` designed to reduce skewness.\n",
    "\n",
    "* **Price & Desc Length Norm (0.058):**\n",
    "  The correlation between price and description length (normalized) is very weak and positive. This indicates that longer product descriptions have a very slight tendency to be associated with higher prices, but the effect is minimal.\n",
    "\n",
    "* **Price & Item Condition (0.005):**\n",
    "  Price shows almost no linear relationship with item condition. This suggests item condition might not directly influence price in a linear manner, or the effect is negligible in this dataset.\n",
    "\n",
    "* **Price & Shipping (-0.111):**\n",
    "  There is a weak negative correlation between price and shipping. This suggests items with shipping fees might tend to have slightly lower prices, or cheaper items might have shipping included, but the effect is not very strong.\n",
    "\n",
    "* **Item Condition & Desc Length (-0.120):**\n",
    "  There is a weak negative correlation between item condition and description length, implying that better condition items might have slightly shorter descriptions, but this is a subtle relationship.\n",
    "\n",
    "* **Shipping & Log Price Norm (-0.233):**\n",
    "  There is a mild negative correlation between shipping and the log price, stronger than with raw price, which again suggests that items requiring shipping fees tend to be lower priced on a log scale.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "\n",
    "* **Strongest relationships:** Between price and log\\_price\\_norm (as expected, since one is a transformation of the other).\n",
    "* **Weak to negligible relationships:** Between price and other features like description length, item condition, and shipping.\n",
    "* These weak correlations suggest that features other than the ones analyzed here may be important predictors of price, or relationships may be non-linear and require more advanced modeling techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building and Prediction (4M)\n",
    "\n",
    "Fit a linear regression model using the most important features identified(1M).Plot the visuals(1M).Briefly explain the regression model,equation (1M) and perform one prediction using the same(1M)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Modeling\n",
    "\n",
    "* **Objective:** Extract the selected features from the standardized dataframe (df_std) and set the target variable (y). Standardize numeric features to have zero mean and unit variance for better model convergence and performance, while preserving categorical features.\n",
    "\n",
    "* **Steps:**\n",
    "\n",
    "  1. **Identify numeric columns** (e.g., IDs, condition, shipping, description length, encoded brand).\n",
    "  2. **Extract categorical columns** as those not in the numeric list.\n",
    "  3. Convert categorical columns to string and then to numeric (if one-hot encoded), replacing non-convertible values with zero.\n",
    "  4. Apply `StandardScaler` to numeric columns to standardize their scale.\n",
    "  5. Concatenate scaled numeric features and processed categorical features into one final dataframe ready for modeling.\n",
    "\n",
    "* **Outcome:** A combined, standardized dataset (`df_std`) suitable for feeding into regression or other machine learning models, ensuring all features are numeric and properly scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define numeric columns to scale\n",
    "numeric_cols_to_scale = ['train_id', 'item_condition_id', 'shipping', 'desc_len', 'desc_len_norm', 'brand_name_le']\n",
    "\n",
    "# 2. Identify categorical columns as all other columns not in numeric_cols_to_scale\n",
    "cat_cols = df_norm_encoded.columns.difference(numeric_cols_to_scale)\n",
    "\n",
    "# 3. Convert categorical columns to string type to avoid categorical dtype issues\n",
    "df_cat = df_norm_encoded[cat_cols].astype(str).fillna('0')\n",
    "\n",
    "# If these are one-hot encoded columns, convert them to numeric\n",
    "# Try to convert to numeric, non-convertible become NaN\n",
    "df_cat = df_cat.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "# 4. Scale numeric features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_num_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df_norm_encoded[numeric_cols_to_scale]),\n",
    "    columns=numeric_cols_to_scale,\n",
    "    index=df_norm_encoded.index\n",
    ")\n",
    "\n",
    "# 5. Combine scaled numeric and categorical features into one dataframe\n",
    "df_std = pd.concat([df_num_scaled, df_cat], axis=1)\n",
    "\n",
    "# 6. Display shape and preview\n",
    "print(f\"Combined standardized dataframe shape: {df_std.shape}\")\n",
    "print(df_std.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of Most Important Features\n",
    "\n",
    "* Based on feature selection techniques (Chi-square and Mutual Information), the following key features were identified as most relevant for predicting price:\n",
    "\n",
    "  * `desc_len` (Description length)\n",
    "  * `shipping` (Shipping option)\n",
    "  * `brand_name_le` (Encoded brand name)\n",
    "  * `item_condition_id` (Product condition)\n",
    "  * `log_price_norm` (Normalized log-transformed price â€” target or reference)\n",
    "\n",
    "* These features capture important aspects such as product description detail, shipping method, brand influence, item condition, and target scaling.\n",
    "\n",
    "* Using this subset simplifies the model, improves interpretability, and reduces overfitting risk while maintaining predictive power.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the Most Important Features\n",
    "selected_features = [\n",
    "    'desc_len', \n",
    "    'shipping', \n",
    "    'brand_name_le', \n",
    "    'item_condition_id', \n",
    "    'log_price_norm'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data for Regression\n",
    "\n",
    "* **Target Variable (`y`)**: The actual product price from the standardized dataframe.\n",
    "\n",
    "* **Feature Matrix (`X`)**: The selected important features extracted from the standardized dataframe.\n",
    "\n",
    "* **Train-Test Split**: The dataset is split into training (80%) and testing (20%) sets to evaluate model performance on unseen data, ensuring generalization.\n",
    "\n",
    "* **Random State**: Set for reproducibility of the split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable\n",
    "y = df_std['price']\n",
    "\n",
    "# Feature matrix\n",
    "X = df_std[selected_features]\n",
    "\n",
    "# Split data: 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluating the Linear Regression Model\n",
    "\n",
    "* **Model Initialization and Training:**\n",
    "  A `LinearRegression` model is instantiated and trained on the training data (`X_train`, `y_train`), learning the linear relationships between selected features and the target price.\n",
    "\n",
    "* **Prediction:**\n",
    "  The trained model predicts prices on the test dataset (`X_test`), generating `y_pred`.\n",
    "\n",
    "* **Evaluation Metrics:**\n",
    "\n",
    "  * **Mean Squared Error (MSE):** Measures the average squared difference between actual and predicted prices; lower values indicate better fit.\n",
    "  * **RÂ² Score:** Indicates the proportion of variance in the target variable explained by the model; values closer to 1 imply better predictive power.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"RÂ² Score: {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual vs Predicted Prices Plot\n",
    "\n",
    "* **Scatter Plot:** Visualizes how well predicted prices (`y_pred`) align with the actual prices (`y_test`). Each point represents one test sample.\n",
    "\n",
    "* **Ideal Fit Line (y = x):** The dashed red line shows perfect predictions where predicted price equals actual price. Points close to this line indicate accurate predictions.\n",
    "\n",
    "* **Interpretation:**\n",
    "\n",
    "  * Points tightly clustered around the red line suggest the model predicts prices well.\n",
    "  * Deviations highlight prediction errors or areas where the model struggles.\n",
    "\n",
    "* **Purpose:** This visualization helps quickly assess the modelâ€™s accuracy and spot any systematic over- or under-predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=y_test, y=y_pred, alpha=0.5)\n",
    "\n",
    "# Plot ideal prediction line (y = x)\n",
    "lims = [min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())]\n",
    "plt.plot(lims, lims, '--r', label='Ideal Fit (y = x)')\n",
    "\n",
    "plt.xlabel(\"Actual Prices\")\n",
    "plt.ylabel(\"Predicted Prices\")\n",
    "plt.title(\"Actual vs Predicted Prices (Linear Regression)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Actual vs Predicted Plot\n",
    "\n",
    "* The **red dashed line (y = x)** represents the ideal scenario where predicted prices exactly match the actual prices.\n",
    "* **Points above the line** indicate **underestimation** â€” the model predicted a lower price than actual.\n",
    "* **Points below the line** indicate **overestimation** â€” the model predicted a higher price than actual.\n",
    "* This plot provides a straightforward visual to assess prediction accuracy and bias.\n",
    "* Adding this reference line greatly enhances interpretability in regression evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Linear Regression and Model Equation\n",
    "Linear Regression is a fundamental supervised learning algorithm used for predicting a continuous target variableâ€”in this case, the product price. The goal of linear regression is to model the relationship between one or more explanatory variables (features) and the target by fitting a linear equation to observed data.\n",
    "The general form of the linear regression model is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_n x_n + \\varepsilon\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $$( \\hat{y} )$$is the **predicted value** (in this case, the price).\n",
    "\n",
    "- $$( \\beta_0 ) $$ is the **intercept** â€” the expected value of y when all input features are zero.\n",
    "\n",
    "- $$( \\beta_1, \\beta_2, \\ldots, \\beta_n )$$ are the **coefficients** associated with each feature $$( x_1, x_2, \\ldots, x_n )$$\n",
    " \n",
    "- $$( \\varepsilon )$$ is the **error term**, representing the difference between the observed and predicted values.\n",
    "\n",
    "\n",
    "For our case, the model can be written as:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Price} = \\beta_0 + \\beta_1 \\cdot \\text{desc\\_len} + \\beta_2 \\cdot \\text{shipping} + \\beta_3 \\cdot \\text{brand\\_name\\_le} + \\beta_4 \\cdot \\text{item\\_condition\\_id} + \\beta_5 \\cdot \\text{log\\_price\\_norm} + \\varepsilon\n",
    "$$\n",
    "\n",
    "This linear model assumes that the relationship between each feature and the target variable is additive and linear. The coefficients (Î²) are learned from the training data to minimize the prediction error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Prediction Demonstration\n",
    "\n",
    "This code selects a single random sample from the test dataset and uses the trained linear regression model to predict its price. It then prints both the input feature values for this sample and compares the predicted price with the actual price. This helps illustrate how the model performs on individual data points and provides a tangible example of its predictive ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random sample from test set\n",
    "sample = X_test.iloc[[0]]\n",
    "predicted_price = lr_model.predict(sample)\n",
    "\n",
    "print(\"Sample input:\")\n",
    "print(sample)\n",
    "print(f\"\\nPredicted Price: ${predicted_price[0]:.2f}\")\n",
    "print(f\"Actual Price: ${y_test.iloc[0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations and Conclusions(1M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Observations**:\n",
    "\n",
    "#### 1. **Model Fit**:\n",
    "\n",
    "   * The regression model was trained on standardized features, ensuring comparability across all input variables.\n",
    "   * Evaluation using an **Actual vs Predicted scatter plot** shows most points lie close to the ideal line (`y = x`), suggesting a reasonably good fit.\n",
    "\n",
    "#### 2. **Error Trend**:\n",
    "\n",
    "   * Some scatter is visible, especially at higher price values, indicating the model has **slightly more error for expensive products**.\n",
    "   * Lower-priced items are predicted with higher accuracy.\n",
    "\n",
    "#### 3. **Feature Influence**:\n",
    "\n",
    "   * Key features like `log_price_norm`, `shipping`, `desc_len`, and `brand_name_le` had high MI scores, validating their strong influence on the target variable.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusions**:\n",
    "\n",
    "* **Linear Regression** provides a **straightforward and interpretable model**, which worked fairly well on the selected features.\n",
    "* The presence of minor deviations suggests that the model might benefit from more complex algorithms (e.g., decision trees or ensemble methods) for further accuracy.\n",
    "* Overall, for the scope of this analysis, linear regression serves as an effective baseline model, highlighting the key factors that impact product price on the platform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Solution (1M)\n",
    "\n",
    "What is the solution that is proposed to solve the business problem discussed in the beginning. Also share your learnings while working through solving the problem in terms of challenges, observations, decisions made etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Proposed Solution to the Business Problem**\n",
    "\n",
    "The business objective was to **predict product prices** based on listing features such as product descriptions, item condition, category hierarchy, shipping information, and brand metadata. Accurate price prediction can improve user experience, support dynamic pricing strategies, and assist in inventory decisions.\n",
    "\n",
    "To address this, a **Linear Regression model** was developed using a **sample of 10,000 rows** from the Mercari dataset to ensure processing efficiency and memory feasibility. The following steps were undertaken:\n",
    "\n",
    "---\n",
    "\n",
    "#### **Data Preprocessing**\n",
    "\n",
    "* Cleaned and filtered the data (no duplicates were present).\n",
    "* Filled missing values with `\"Unknown\"` for categorical variables and median for numerics.\n",
    "* Standardized string formats (lowercased, stripped).\n",
    "* Cleaned text in `item_description` and engineered new features such as `desc_len`.\n",
    "* Normalized and log-transformed `price` to reduce skewness.\n",
    "* Split hierarchical categories (`category_name`) into three levels: `cat_1`, `cat_2`, and `cat_3`.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Feature Engineering & Selection**\n",
    "\n",
    "* Applied **equal-frequency binning** to discretize `price` and `desc_len`.\n",
    "* One-hot encoded selected categorical variables (e.g., binned categories, category splits).\n",
    "* Used **Chi-Square (Ï‡Â²)** for categorical feature selection and **Mutual Information (MI)** for both categorical and numeric variables.\n",
    "* To avoid crashes from memory exhaustion, MI was computed on the **10,000-sample subset** only.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Data Transformation**\n",
    "\n",
    "* Standardized selected numeric features using `StandardScaler`.\n",
    "* Combined scaled numerical features with processed categorical features to create a model-ready dataframe.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Modeling**\n",
    "\n",
    "* Built a **Linear Regression model** using the top features selected (e.g., `desc_len`, `shipping`, `brand_name_le`, `item_condition_id`, `log_price_norm`).\n",
    "* Split data into 80% training and 20% testing for validation.\n",
    "* Trained the model and evaluated performance using **MSE** and **RÂ² Score**.\n",
    "* Visualized **actual vs. predicted prices** with scatter plots and an ideal prediction line (`y = x`).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Evaluation & Prediction**\n",
    "\n",
    "* Achieved a baseline regression model that predicts price with acceptable error and interpretability.\n",
    "* Ran **individual sample predictions** to demonstrate real-time usability of the model.\n",
    "\n",
    "---\n",
    "\n",
    "### **Learnings and Reflections**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Challenges Faced**\n",
    "\n",
    "1. **High Dimensionality & Memory Constraints**\n",
    "\n",
    "   * One-hot encoding and large data volume led to **MemoryErrors**.\n",
    "   * Scaling operations converted sparse data to dense `float64`, overwhelming system memory.\n",
    "\n",
    "2. **Mutual Information Crash**\n",
    "\n",
    "   * MI regression failed on the full dataset due to internal transformations.\n",
    "   * **Workaround**: Reduced data to a 10,000-row subset to complete MI analysis.\n",
    "\n",
    "3. **Data Cleaning Complexity**\n",
    "\n",
    "   * Required type-casting of categories and handling placeholder text.\n",
    "   * Addressed inconsistencies in price, shipping, and categorical splits.\n",
    "   * Applied transformations carefully to avoid dtype conflicts during encoding or scaling.\n",
    "\n",
    "4. **Skewed Target Variable**\n",
    "\n",
    "   * Raw prices were heavily right-skewed; applied **log transformation** to stabilize distribution for regression.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Observations**\n",
    "\n",
    "* **Shipping**, **item condition**, and **brand\\_name** were influential in predicting price.\n",
    "* **Log-transformed prices** improved regression performance and interpretability.\n",
    "* **Chi-Square** was effective for categorical filtering, while **Mutual Information** captured non-linear dependencies.\n",
    "* Visualizations like **boxplots**, **scatter plots**, and **correlation heatmaps** revealed insightful data relationships.\n",
    "\n",
    "---\n",
    "\n",
    "### **Decisions Made**\n",
    "\n",
    "* Performed **sampling** (10,000 rows) to avoid crashes while preserving feature diversity.\n",
    "* Used **standardization** post feature selection to ensure consistent scaling.\n",
    "* Excluded high-cardinality or uninformative fields (e.g., `train_id`, `item_description`).\n",
    "* Chose **Linear Regression** due to project constraints and the requirement for interpretability, while leaving scope for future modeling enhancements.\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Outcome**\n",
    "\n",
    "A lightweight yet interpretable **Linear Regression model** was developed and tested successfully. It allows the business to:\n",
    "\n",
    "* Offer **data-driven price suggestions** to sellers.\n",
    "* **Detect and flag pricing anomalies**.\n",
    "* **Automate price recommendations** for new product listings.\n",
    "\n",
    "This model sets a strong foundation for future enhancements such as advanced tree-based models (e.g., Random Forest, XGBoost) or deep learning pipelines, should computational resources allow.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
